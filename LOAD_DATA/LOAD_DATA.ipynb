{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "e7dtt4beif65ohagrhgt",
   "authorId": "",
   "authorName": "",
   "sessionId": "fae3e35b-a968-4d5d-9302-a67235adc7f6",
   "lastEditTime": 0
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6482415-2fa5-403e-81cf-fb8515bb5c07",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Load data into Snowflake tables\n",
    "To begin, download the full dataset (1 million rows) as a zip from this [Kaggle link](https://www.kaggle.com/datasets/sridharstreaks/insurance-data-for-machine-learning). Then unzip it to a .csv. Load that .csv into your notebook's files directory. We will pull from that csv to create a training data table and our incoming \"streamed\" data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import pandas as pd\n",
    "\n",
    "# Create the session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666727f-66aa-4176-a580-f9f4d84f0420",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": [
    "Load data from csv into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b78a9-9ab3-4995-9884-c381688bc382",
   "metadata": {
    "name": "cell6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load full 1M dataset into dataframe\n",
    "insurance_df = pd.read_csv('insurance_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f409dc-8534-4944-b481-4aaa660a73cb",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "Data cleaning, rearranging columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde22f4-f7f3-4f17-80f3-19e7399811bf",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# Capitalize column names\n",
    "insurance_df.columns = insurance_df.columns.str.upper()\n",
    "\n",
    "# Rearrange columns to fit target schema\n",
    "cols = insurance_df.columns.tolist()\n",
    "cols = cols[:3] + cols[-1:] + cols[3:-1]\n",
    "insurance_df = insurance_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93962c-c871-4eaa-9a55-68e8160c38e4",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "Use the write_pandas() method to write the first 10k rows into the 'SOURCE_OF_TRUTH' table created with the SQL commands in the SQL file. The method \"returns a Snowpark DataFrame object referring to the table where the pandas DataFrame was written to.\" (Snowpark Documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608a983-20ec-48a0-a38e-d85f900476fa",
   "metadata": {
    "name": "cell9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "source_of_truth_df = session.write_pandas(insurance_df[:10000], table_name='SOURCE_OF_TRUTH',database='INSURANCE',schema='ML_PIPE',auto_create_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b730e-6865-42e5-84ed-11468754e045",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "The code below writes the remaining 990k to the INCOMING_DATA_SOURCE table to simulate data being streamed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137111d-ef9c-4bad-8e49-4edfbfe3189c",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "incoming_data_source_df = session.write_pandas(insurance_df[10000:], table_name='INCOMING_DATA_SOURCE',database='INSURANCE',schema='ML_PIPE',auto_create_table=True)"
   ]
  }
 ]
}